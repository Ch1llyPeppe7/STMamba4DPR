{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSetInitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from Modules.mamba4poi import Mamba4POI\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=Mamba4POI, config_file_list=['config.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_user=(dataset.inter_feat[dataset.uid_field].max()+1).astype(int)\n",
    "num_category=(dataset.item_feat[\"venue_category_id\"].max()+1).astype(int)\n",
    "num_POI=(dataset.item_feat[\"venue_id\"].max()+1).astype(int)\n",
    "\n",
    "Ms=torch.zeros((num_user,num_category),dtype=torch.float32)\n",
    "_,M0,_,_=counting4all(train_data._dataset,device)\n",
    "\n",
    "Ec=torch.zeros((num_category,k),dtype=torch.float32)\n",
    "ESu=torch.zeros((num_user,k),dtype=torch.float32)\n",
    "\n",
    "itemX=(dataset.item_feat[\"longitude\"]).to_numpy()\n",
    "itemY=(dataset.item_feat[\"latitude\"]).to_numpy()\n",
    "itemC=(dataset.item_feat[\"venue_category_id\"]).to_numpy().astype(np.int)\n",
    "Locations=torch.tensor(np.stack((itemX,itemY),axis=-1),dtype=torch.double)\n",
    "\n",
    "\n",
    "row_nonzero_mask=M0.sum(1)>0\n",
    "col_nonzero_mask=M0.sum(0)>0\n",
    "nonzero_M0=M0[row_nonzero_mask][:,col_nonzero_mask]\n",
    "\n",
    "E0u,S0,E0c=UC_SVD(nonzero_M0,k)\n",
    "\n",
    "Ec[col_nonzero_mask]=E0c\n",
    "\n",
    "EPOI=torch.einsum('ij,ik->ijk',Ec[itemC],Locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_data:\n",
    "    userids,item_seqs,Xs,Ys,Cs,labels,times,negs=batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ms=accumulate_category(Cs,userids,Ms)\n",
    "s_row_nonzero_mask=Ms.sum(1)>0\n",
    "s_col_nonzero_mask=Ms.sum(0)>0\n",
    "nonzero_Ms=Ms[s_row_nonzero_mask][:,s_col_nonzero_mask]\n",
    "\n",
    "row,col=nonzero_Ms.shape\n",
    "size = max(row, col)  \n",
    "expanded_Ms = torch.zeros(size, size)\n",
    "expanded_Ms[:row,:col]=nonzero_Ms\n",
    "if row==size:\n",
    "    expanded_Ms[:,col:]=torch.eye(row,row - col) * 1e5\n",
    "else:\n",
    "    expanded_Ms[row:,:]=torch.eye(col-row,col)*1e5\n",
    "\n",
    "Esu,Ss,Esc=UC_SVD(expanded_Ms,k)\n",
    "\n",
    "Estu=Ec[s_col_nonzero_mask]@Esc.T@Esu\n",
    "ESu[s_row_nonzero_mask]=Estu[:row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=np.stack((Xs,Ys),axis=-1)\n",
    "El=torch.tensor(location,dtype=torch.double)\n",
    "Ep=torch.einsum('usj,usk->usjk',Ec[Cs],El)#numuserXnumseqXcategoryfeatureXLocationfeature\n",
    "S=torch.einsum('ut,usjk->ustjk',ESu[userids[:,0]],Ep)#numuserXnumseqXcategoryfeatureXLocationfeatureXUserFeat\n",
    "labels=torch.tensor(np.array(labels))\n",
    "valid_S=S[(labels==1)|(labels==2)].reshape(S.shape[0],-1,S.shape[2],S.shape[3],S.shape[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times=torch.tensor(times,dtype=torch.double)\n",
    "dT=(Times[:,1:]-Times[:,:-1]).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
    "S1=valid_S[:,1:]/dT\n",
    "S2=valid_S[:,:-1]/dT\n",
    "\n",
    "shape = S1.shape\n",
    "\n",
    "new_shape = (shape[0], shape[1] * 2, shape[2], shape[3], shape[4])\n",
    "\n",
    "Ss = torch.zeros(new_shape,dtype=torch.double)\n",
    "Ss[:,0::2]=S2\n",
    "Ss[:,1::2]=S1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "for obj in gc.get_objects():\n",
    "    if torch.is_tensor(obj) and obj.is_cuda:\n",
    "        print(type(obj), obj.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperGraphModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notes:\n",
    "We use a **HyperGraph Neural Network** to model the complex interactions among users, categories, and POIs, calculating the weight of hyperedges to reflect users' **interests in both categories and POIs**.<br>\n",
    "Unlike our sequential prediction model, which predict user interest in the dynamic time sequence, the HGNN addresses the issue of dissimilar interests in the same POI caused by users' varied **breadth of interests** despite having equal interaction levels,by modeling static user profiles.The **distinction** between the two modules is determined by their approach to **utilizing interactions**: ***dynamically or statically***<br>\n",
    "#### Nodes defination:\n",
    "We detatch category from POI features to modeling its latent<br> relation between both POI and user<br><br>\n",
    "**User**:       10dims for 3 features<br>\n",
    "*num_poi_interaction*:int,*num_category_interaction*:int<br>\n",
    "***active_area:tensor***(4,2) <--training target<br><br>\n",
    "**Category**:   1dim for 1 feature<br>\n",
    "*num_poi*:int,sigma:float(2,),center:float(2,)<br>\n",
    "***using poi geographic distribution feature to reflect the feature of category***<br>\n",
    "<br><br>\n",
    "**POI**:        2dims for 1feature<br>\n",
    "*location*:tensor(2,)<br>\n",
    "\n",
    "             \n",
    "\n",
    "#### Weight updating policy:\n",
    "Our Hyperedge connected with three varied type nodes,whose weight can't be fully normalized as scalar,are designed with vector weight.Using attention mechanism to reflect the diffrent impact from nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "def CreateAdjacencyList(locations):\n",
    "    radius=200\n",
    "    POI_tree=KDTree(locations)\n",
    "    adjacency_list = POI_tree.query_ball_point(locations,r=radius,p=1)\n",
    "\n",
    "    return adjacency_list\n",
    "\n",
    "def adjacency_list_to_edge_index(adjacency_list,itemC,num_poi):\n",
    "    #GroupIdx=[]\n",
    "    edge_index = []\n",
    "    catedge_index = []\n",
    "    C=itemC+num_poi\n",
    "    for src_node, neighbors in enumerate(adjacency_list):\n",
    "       #GroupIdx.append(len(edge_index))\n",
    "        for tgt_node in neighbors:\n",
    "                edge_index.append([src_node, tgt_node])\n",
    "        #GroupIdx.append(len(edge_index)-GroupIdx[-1])\n",
    "        catedge_index.append([src_node,C[src_node]])\n",
    "        catedge_index.append([C[src_node],src_node])\n",
    "    turnidx=len(edge_index)\n",
    "    edge_index = torch.tensor(edge_index+catedge_index, dtype=torch.int)\n",
    "    return edge_index.T,turnidx#,GroupIdx\n",
    "\n",
    "locations=np.stack((itemX,itemY),axis=-1)\n",
    "num_poi=locations.shape[0]\n",
    "adjacency_list=CreateAdjacencyList(locations)\n",
    "CnitemC=np.concatenate([itemC, np.arange(num_category)])\n",
    "edge_index,turnidx= adjacency_list_to_edge_index(adjacency_list,CnitemC,num_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "edgeMLP = nn.Sequential(\n",
    "            nn.Linear(10+50+2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        ).to(device)\n",
    "node_feats = torch.randn(location.shape[0]+num_POI, 10, requires_grad=True).to(device) # 可学习嵌入\n",
    "locations = torch.tensor(location, dtype=torch.float32).to(device)\n",
    "edge_index=edge_index.to(device)\n",
    "Ec=Ec.to(device)\n",
    "CnitemC=torch.tensor(CnitemC).to(device)\n",
    "for idx in range(0,turnidx,1000):\n",
    "            loc_diff=torch.abs(locations[edge_index[idx:min(idx+1000,turnidx),0]]- locations[edge_index[idx:min(idx+1000,turnidx),1]])\n",
    "            cat_diff=torch.abs(Ec[CnitemC[edge_index[idx:min(idx+1000,turnidx),0]]]-Ec[CnitemC[edge_index[idx:min(idx+1000,turnidx),1]]])\n",
    "            embedding_diff = torch.abs(node_feats[edge_index[idx:min(idx+1000,turnidx),0]] - node_feats[edge_index[idx:min(idx+1000,turnidx),1]])\n",
    "            \n",
    "            edge_feat=torch.cat([loc_diff,cat_diff,embedding_diff], dim=1)\n",
    "            \n",
    "       \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "def PositionEmbedding(positions, output_dim, noise_std=1, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # 原始位置张量\n",
    "    N, input_dim = positions.shape\n",
    "\n",
    "    \n",
    "    normalized_positions=positions/positions.max(0).values\n",
    "\n",
    "    # 添加高斯噪声\n",
    "    noise = torch.randn(N,output_dim-input_dim) * noise_std\n",
    "    el = torch.concat([normalized_positions,noise],dim=1)\n",
    "    \n",
    "    return el\n",
    "# -------------------------------\n",
    "# 1. 超图卷积层\n",
    "# -------------------------------\n",
    "class HeteroHyperGCN(MessagePassing):\n",
    "    def __init__(self, node_dim, cat_dim, hidden_dim, edge_dim, output_dim,CnitemC,batchsize):\n",
    "        super(HeteroHyperGCN, self).__init__(aggr='add')  # 聚合方式为求和\n",
    "        self.node_dim = node_dim\n",
    "        self.cat_dim = cat_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.CnitemC=CnitemC\n",
    "        self.BatchSize=batchsize\n",
    "\n",
    "\n",
    "        # 边权重更新 MLP\n",
    "        self.poiMLP = nn.Sequential(\n",
    "            nn.Linear(2*node_dim+cat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, node_feats, edge_index,turnidx,locations,Ec):\n",
    "        edge_weights = self.compute_edge_weights(node_feats, edge_index,turnidx,locations,Ec)\n",
    "        updated_feats = self.propagate(edge_index, x=node_feats, edge_weight=edge_weights)\n",
    "        return updated_feats, edge_weights\n",
    "\n",
    "    def compute_edge_weights(self, node_feats, edge_index,turnidx,locations,Ec):\n",
    "        edge_weights = []\n",
    "        for idx in range(0,turnidx,1000):\n",
    "            end_idx=min(idx+1000,turnidx)\n",
    "            loc_diff=torch.abs(locations[edge_index[0,idx:end_idx]]- locations[edge_index[1,idx:end_idx]])\n",
    "            cat_diff=torch.abs(Ec[self.CnitemC[edge_index[0,idx:end_idx]]]-Ec[self.CnitemC[edge_index[1,idx:end_idx]]])\n",
    "            embedding_diff =node_feats[edge_index[0,idx:end_idx]] - node_feats[edge_index[1,idx:end_idx]]\n",
    "            \n",
    "            edge_feat=torch.cat([loc_diff,cat_diff], dim=1)\n",
    "            \n",
    "            weight=embedding_diff*self.poiMLP(edge_feat)\n",
    "\n",
    "            edge_weights.append(weight)\n",
    "\n",
    "        for idx in range(turnidx,locations.shape[0],1000):\n",
    "            end_idx=min(idx+1000,turnidx)\n",
    "            embedding_diff = torch.abs(node_feats[edge_index[0,idx:end_idx]] - node_feats[edge_index[1,idx:end_idx]])\n",
    "            decay=0.01\n",
    "            weight=decay*embedding_diff\n",
    "            edge_weights.append(weight)\n",
    "\n",
    "\n",
    "        return torch.cat(edge_weights, dim=0)\n",
    "\n",
    "    def message(self,x_j, edge_weight=None):\n",
    "        return edge_weight \n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        return torch.scatter(inputs, index, dim=0, reduce=self.aggr)\n",
    "\n",
    "    def update(self, inputs,x):\n",
    "        # 更新节点嵌入\n",
    "        return inputs+x\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 模型定义\n",
    "# -------------------------------\n",
    "class HeteroGraphModel(nn.Module):\n",
    "    def __init__(self, node_dim, cat_dim, hidden_dim, edge_dim, output_dim,CnitemC,batchsize=1000):\n",
    "        super(HeteroGraphModel, self).__init__()\n",
    "        self.hyper_gcn = HeteroHyperGCN(node_dim, cat_dim, hidden_dim, edge_dim, output_dim,CnitemC,batchsize)\n",
    "        self.embedding = nn.Embedding(node_dim, output_dim)\n",
    "\n",
    "    def forward(self, node_feats, edge_index,turnidx,locations,Ec):\n",
    "        node_feats, edge_weights = self.hyper_gcn(node_feats, edge_index,turnidx,locations,Ec)\n",
    "        return node_feats, edge_weights\n",
    "\n",
    "    def loss(self, node_feats, edge_index,turnidx,locations,Ec):\n",
    "       total_loss=0\n",
    "       for idx in range(0,turnidx,1000):\n",
    "            end_idx=min(idx+1000,turnidx)\n",
    "            loc_diff=torch.abs(locations[edge_index[idx:end_idx,0]]- locations[edge_index[idx:end_idx,1]])\n",
    "            cat_diff=torch.abs(Ec[self.CnitemC[edge_index[idx:end_idx,0]]]-Ec[self.CnitemC[edge_index[idx:end_idx,1]]])\n",
    "            embedding_diff = torch.abs(node_feats[edge_index[idx:end_idx,0]] - node_feats[edge_index[idx:end_idx,1]])\n",
    "            batch_loss=embedding_diff/(loc_diff/cat_diff)\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. 数据加载与初始化\n",
    "# -------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "node_dim=10, cat_dim=50 ,hidden_dim=32, edge_dim=1, output_dim=20\n",
    "\n",
    "locations=np.stack((itemX,itemY),axis=-1)[:20000]\n",
    "num_poi=locations.shape[0]\n",
    "\n",
    "adjacency_list=CreateAdjacencyList(locations)\n",
    "locations = torch.tensor(location, dtype=torch.float32).to(device)\n",
    "\n",
    "CnitemC=np.concatenate([itemC, np.arange(num_category)])\n",
    "CnitemC=torch.tensor(CnitemC).to(device)\n",
    "\n",
    "edge_index,turnidx= adjacency_list_to_edge_index(adjacency_list,CnitemC,num_poi)\n",
    "\n",
    "Ec=Ec.to(device)\n",
    "\n",
    "node_feats = PositionEmbedding(location,)\n",
    "\n",
    "\n",
    "# 模型与优化器\n",
    "model = HeteroGraphModel(node_dim=node_dim, cat_dim=cat_dim ,hidden_dim=hidden_dim, edge_dim=edge_dim, output_dim=output_dim,CnitemC=CnitemC).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 训练循环\n",
    "# -------------------------------\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    node_embeds, edge_weights = model(node_feats, edge_index.to(device),turnidx, locations,Ec)\n",
    "    # 你可以在这里添加损失函数，例如基于边权重或嵌入对比损失\n",
    "    loss = torch.sum(edge_weights)  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venue Category Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 负采样优化\n",
    "空间融合基本方法：基于流行度采样之上，候选负样本缩小到用户活动范围内，目的是排除活动范围外流行的伪负样本\n",
    "活动范围：用户交互过的地点位置取最大外切矩形\n",
    "分层采样：依据交互次数排行 找到频繁访问区域作为生活区 最少访问区域作为危险区 其余部分作为探索区\n",
    "假设1：用户熟知生活区所有地点，因此没有访问的地点作为负样本\n",
    "假设2：危险区可能是用户真的不感兴趣的区域 也有可能是不方便涉足的区域，可以根据危险区和生活区的距离进行从分段处理控制采样比例 危险区采样比例根据其与生活区距离\n",
    "探索区域：融合类目分布进行采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物品的特征可以这样观察 冷门-流行 丰富-稀有 再加上用户访问次数特征 构建出一个三维空间\n",
    "丰富稀有可以转化成 日常化程度 \n",
    "#### 假设1：相比更日常的地点人们对稀有的地点更感兴趣\n",
    "#### 假设2：用户更多访问的地方可能不是因为感兴趣而是因为日常生活需要\n",
    "#### 假设3：用户的频繁访问点是集中的，用户频繁访问区内的地点用户都熟知，未访问的地点大概率是不感兴趣\n",
    "#### 假设4：用户频繁访问区外的用户的偶尔访问点是集中的，用户偶尔访问区根据与频繁访问区的距离可以感知用户是对这块区域没兴趣还是不方便探索，这个距离是根据整体区域大小裁定的\n",
    "#### 假设5：除了偶尔访问区和频繁访问区，剩下的探索区是很未知的，\n",
    "#### 假设6：根据频繁访问区每个类目 用户访问占区域内总数比例 粗略推断用户对类目的兴趣 把这个权重作用在探索区\n",
    "#### 策略1：根据全体交互item划分出总区域\n",
    "#### 策略2：根据分位数划分出频繁访问item，在划分出频繁访问去地理包络，记录包络中心点位置，包络中未访问的项目直接划分成候选负样本\n",
    "#### 策略3：在频繁访问区地理包络外根据分位数划分出偶尔访问item，得到偶尔访问区地理包络，以中心位置到频繁访问区中心位置距离和总区域斜边两者倒数比来度量相近程度作为偶尔访问区采样权重\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "POI_interaction_matrix,category_interaction_matrix,category_ids_counts=counting4all(dataset,device)\n",
    "\n",
    "reciprocalrarity=reciprocal_rarity(category_ids_counts)\n",
    "exprarity=exp_rarity(category_ids_counts,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现象1：活跃用户和非活跃用户的交互分布特征相似 大多数访问的地点去的次数很少 只有少数地点去了很多次 这些地点反映用户兴趣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# 假设 interaction_matrix 是给定的张量\n",
    "# Step 1: Identify the most active user (column with max interactions excluding the first row and column)\n",
    "active_user_idx = category_interaction_matrix[1:, 1:].sum(1).argmax()\n",
    "active_user = category_interaction_matrix[active_user_idx+1,:].cpu().numpy()\n",
    "\n",
    "# Step 2: Identify the least active user (column with min interactions excluding the first row and column)\n",
    "inactive_user_idx = category_interaction_matrix[1:, 1:].sum(1).argmin()\n",
    "inactive_user = category_interaction_matrix[inactive_user_idx+1,:].cpu().numpy()\n",
    "\n",
    "# 计算均值\n",
    "mean_interaction = category_interaction_matrix[1:,1:].mean(dtype=torch.float32)\n",
    "ordinary_user_idx = (abs(category_interaction_matrix[1:,1:] - mean_interaction)).argmin()\n",
    "ordinary_user=category_interaction_matrix[ordinary_user_idx+1,:].cpu().numpy()\n",
    "\n",
    "# Step 3: Filter out zeros\n",
    "active_user = active_user[active_user > 0]\n",
    "inactive_user = inactive_user[inactive_user > 0]\n",
    "ordinary_user=ordinary_user[ordinary_user>0]\n",
    "\n",
    "# Step 4: Create DataFrames for Plotly\n",
    "active_df = pd.DataFrame({\"Interactions\": active_user, \"User Type\": \"Active User\"})\n",
    "inactive_df = pd.DataFrame({\"Interactions\": inactive_user, \"User Type\": \"Inactive User\"})\n",
    "ordinary_df= pd.DataFrame({\"Interactions\": ordinary_user, \"User Type\": \"Ordinary User\"})\n",
    "\n",
    "# Combine both for easier plotting\n",
    "combined_df = pd.concat([active_df, inactive_df,ordinary_df])\n",
    "print(active_user.sum(),inactive_user.sum(),ordinary_user.sum())\n",
    "# Step 5: Plot histograms using Plotly\n",
    "\n",
    "fig = px.histogram(\n",
    "    combined_df,\n",
    "    x=\"Interactions\",\n",
    "    color=\"User Type\",\n",
    "    title=(\n",
    "      \"How can users' interests be effectively measured,<br>\"\n",
    "      \"while balancing the trade-off between interest breadth and depth,<br>\"\n",
    "      \"as illustrated by interaction quantities of <br>\"\n",
    "      \"User-Category and Category-POI (user-wise) or User-POI?\"),\n",
    "    labels={\"Interactions\": \"Number of Interactions\", \"User Type\": \"User Type\"},\n",
    "    nbins=100,  # Adjust the number of bins as needed\n",
    ")\n",
    "\n",
    "# Customize the title position and style\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"y\": 0.7,  # Adjust vertical position (1.0 is top, 0.0 is bottom)\n",
    "        \"x\": 0.5,   # Adjust horizontal position (0.5 is centered)\n",
    "        \"xanchor\": \"center\",  # Anchor to center\n",
    "        \"yanchor\": \"top\",     # Anchor to top\n",
    "        \"font\": {\"size\": 16},  # Adjust font size\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：如何区分大量的少交互地点进行兴趣度量 实际上访问次数极多的地点不具有参考价值 它们可能是日常所需，不能很好地反映兴趣\n",
    "方案：把地点访问次数转化成类目访问次数 \n",
    "首先把userid-itemid交互序列转化成userid-categoryid交互序列进行归并 这样地分布更加平滑，访问总次数是一样的，只不过把大量只访问一次的地点归并成访问多次的类目。 \n",
    "这样我们就可以区分访问次数极少的地点 \n",
    "\n",
    "基于类目的兴趣度量：\n",
    "假设：偶尔访问类目和频繁访问类目不能反映兴趣\n",
    "定义用户兴趣类目：排除极端访问次数类目后的类目\n",
    "\n",
    "问题：用户对访问次数相同的类目兴趣度一致吗？应该怎么区分？\n",
    "方案：基于类目稀有度的加权，同样的访问次数，用户很可能对更稀有的类目感兴趣，引入类目稀有度矩阵\n",
    "\n",
    "后续模型修改：\n",
    "引入兴趣广度概念：不同用户以同样次数访问同个地点，反映的兴趣度是不同的，应该依赖用户访问总类目数量进行加权度量，得到更合理的用户-地点相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：如何把类目数量权重映射到稀有度上？\n",
    "直接映射：倒数加权，这样的得到的稀有度是突变的 我希望稀有度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "\n",
    "reciprocal_rarity=reciprocalrarity.cpu()\n",
    "exp_rarity=exprarity.cpu()\n",
    "\n",
    "# 创建图表对象\n",
    "fig = go.Figure()\n",
    "\n",
    "# Reciprocal Rarity Histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=reciprocal_rarity[1:],\n",
    "    nbinsx=1000,\n",
    "    name=\"Reciprocal Rarity\",\n",
    "    marker_color=\"skyblue\",\n",
    "    histnorm='probability',\n",
    "    opacity=0.7,\n",
    "    showlegend=True  # 不显示图例\n",
    "))\n",
    "\n",
    "# Log Rarity Histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=exp_rarity[1:],\n",
    "    nbinsx=500,\n",
    "    name=\"Exp Rarity\",\n",
    "    marker_color=\"lightgreen\",\n",
    "    opacity=0.7,\n",
    "    histnorm='probability',\n",
    "    showlegend=True  # 不显示图例\n",
    "))\n",
    "\n",
    "# 计算方差和峰度\n",
    "reciprocal_var = torch.var(reciprocal_rarity[1:])\n",
    "reciprocal_kurt = kurtosis(reciprocal_rarity[1:])\n",
    "exp_var = torch.var(exp_rarity[1:])\n",
    "exp_kurt = kurtosis(exp_rarity[1:])\n",
    "\n",
    "\n",
    "# 添加外部文本框显示方差和峰度\n",
    "fig.add_annotation(\n",
    "    x=0.5,  # x轴位置，控制文本框水平位置\n",
    "    y=1.15,  # y轴位置，调整文本框在图表的外部\n",
    "    text=f\"Reciprocal Rarity\\nVar: {reciprocal_var:.4e}, Kurtosis: {reciprocal_kurt:.4e}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    align=\"center\",\n",
    "    bgcolor=\"white\",\n",
    "    opacity=0.8,\n",
    "    xref=\"paper\",  # 使用 'paper' 坐标系，让文本框不受图表范围限制\n",
    "    yref=\"paper\"   # 使用 'paper' 坐标系，使文本框在图表外\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.5,  # x轴位置，控制文本框水平位置\n",
    "    y=1.05,  # y轴位置，调整文本框在图表的外部\n",
    "    text=f\"Exp Rarity\\nVar: {exp_var:.4e}, Kurtosis: {exp_kurt:.4e}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    align=\"center\",\n",
    "    bgcolor=\"white\",\n",
    "    opacity=0.8,\n",
    "    xref=\"paper\",  # 使用 'paper' 坐标系，放置在图表外\n",
    "    yref=\"paper\"   # 使用 'paper' 坐标系，放置在图表外\n",
    ")\n",
    "\n",
    "# 更新布局\n",
    "fig.update_layout(\n",
    "    title=\"Histograms of Reciprocal and Log Rarity\",\n",
    "    xaxis_title=\"Value\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    title_font_size=20,\n",
    "    barmode=\"overlay\",  # 使用 \"overlay\" 叠加直方图，\"group\" 可并排显示\n",
    "    showlegend=True    # 隐藏图例\n",
    ")\n",
    "\n",
    "# 显示图表\n",
    "fig.show()#5.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 2. 将数据转为 PyTorch Tensor 并移动到 GPU 上\n",
    "venue_id = torch.tensor(venue_id, dtype=torch.long).cuda()  \n",
    "venue_category = torch.tensor(venue_category, dtype=torch.long).cuda()\n",
    "x = torch.tensor(x, dtype=torch.float32).cuda()  \n",
    "y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "\n",
    "# 3. 设置网格的尺寸（例如：每个网格大小为500x500）\n",
    "grid_width = 500  # 网格宽度\n",
    "grid_height = 500  # 网格高度\n",
    "\n",
    "# 4. 计算网格的数量（基于坐标的最大最小值）\n",
    "x_min, x_max = x.min(), x.max()\n",
    "y_min, y_max = y.min(), y.max()\n",
    "\n",
    "# 计算网格行列数\n",
    "num_x_grids = int((x_max - x_min) // grid_width) + 1\n",
    "num_y_grids = int((y_max - y_min) // grid_height) + 1\n",
    "\n",
    "\n",
    "# 5. 分批次处理\n",
    "batch_size = 10000  # 每批处理的样本数量\n",
    "num_batches = len(venue_id) // batch_size + 1  # 批次数量\n",
    "\n",
    "# 创建稠密张量来暂存更新，最后将其转换为稀疏张量\n",
    "venue_density_matrix = torch.zeros((num_y_grids, num_x_grids), dtype=torch.int32).cuda()\n",
    "category_density_matrix = torch.zeros((num_y_grids, num_x_grids, len(torch.unique(venue_category))), dtype=torch.int32).cuda()\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    # 计算每个批次的索引范围\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(venue_id))\n",
    "    \n",
    "    # 获取当前批次数据\n",
    "    batch_venue_id = venue_id[start_idx:end_idx]\n",
    "    batch_venue_category = venue_category[start_idx:end_idx]\n",
    "    batch_x = x[start_idx:end_idx]\n",
    "    batch_y = y[start_idx:end_idx]\n",
    "\n",
    "    # 计算每个数据点所属的网格\n",
    "    grid_x = ((batch_x - x_min) / grid_width).floor().long()  # 计算 x 对应的网格位置\n",
    "    grid_y = ((batch_y - y_min) / grid_height).floor().long()  # 计算 y 对应的网格位置\n",
    "\n",
    "    # 确保网格索引不超出边界\n",
    "    grid_x = torch.clamp(grid_x, 0, num_x_grids - 1)\n",
    "    grid_y = torch.clamp(grid_y, 0, num_y_grids - 1)\n",
    "\n",
    "    # 使用 scatter_add_ 更新稠密张量的计数\n",
    "    venue_density_matrix.index_put_((grid_y, grid_x), torch.ones(len(grid_y), dtype=torch.int32).cuda(), accumulate=True)\n",
    "    \n",
    "    # 更新类别密度矩阵\n",
    "    for i in range(len(batch_venue_category)):\n",
    "        category_density_matrix[grid_y[i], grid_x[i], batch_venue_category[i]] += 1\n",
    "\n",
    "# 7. 将稠密张量转换为稀疏张量\n",
    "venue_density_matrix_sparse = venue_density_matrix.to_sparse()\n",
    "category_density_matrix_sparse = category_density_matrix.to_sparse()\n",
    "\n",
    "# 8. 打印结果\n",
    "print(\"Venue Density Matrix Sparse:\")\n",
    "print(venue_density_matrix_sparse)\n",
    "print(\"Category Density Matrix Sparse:\")\n",
    "print(category_density_matrix_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "import torch\n",
    "\n",
    "# 假设 venue_density_matrix_sparse 是一个 PyTorch tensor\n",
    "# 你可以根据你实际的场景替换这个变量\n",
    "venue_density_matrix_sparse = torch.rand(num_y_grids, num_x_grids)  # 模拟密度矩阵\n",
    "\n",
    "# 将 x_min 和 y_min 移回 CPU\n",
    "xmin = x_min.cpu().item()\n",
    "ymin = y_min.cpu().item()\n",
    "\n",
    "# 创建空的图形对象\n",
    "fig = go.Figure()\n",
    "\n",
    "# 定义颜色范围，这里选择一个颜色调色板，如蓝色渐变\n",
    "color_scale = 'Blues'  # 可以选择其他颜色范围，如 'Viridis', 'Cividis', 'Inferno'\n",
    "\n",
    "# 使用 PyTorch 的 max 和 min 函数\n",
    "max_density = torch.max(venue_density_matrix_sparse).item()  # 获取最大密度\n",
    "min_density = torch.min(venue_density_matrix_sparse).item()  # 获取最小密度\n",
    "normalized_density = (venue_density_matrix_sparse - min_density) / (max_density - min_density)  # 归一化\n",
    "\n",
    "# 添加网格图层：根据密度值使用颜色\n",
    "for i in range(num_y_grids):\n",
    "    for j in range(num_x_grids):\n",
    "        # 计算网格的边界\n",
    "        grid_x_min = xmin + j * grid_width\n",
    "        grid_x_max = grid_x_min + grid_width\n",
    "        grid_y_min = ymin + i * grid_height\n",
    "        grid_y_max = grid_y_min + grid_height\n",
    "        \n",
    "        # 根据网格的密度值计算颜色\n",
    "        density = normalized_density[i, j].item()  # 转换为 Python 数字\n",
    "        # 使用颜色渐变进行映射\n",
    "        fillcolor = pc.sequential.Blues[int(density * (len(pc.sequential.Blues) - 1))]\n",
    "\n",
    "        # 绘制矩形网格，设置透明度\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[grid_x_min, grid_x_min, grid_x_max, grid_x_max, grid_x_min],\n",
    "            y=[grid_y_min, grid_y_max, grid_y_max, grid_y_min, grid_y_min],\n",
    "            fill='toself',\n",
    "            fillcolor=fillcolor,\n",
    "            line=dict(width=1, color='black'),  # 添加黑色网格线\n",
    "            mode='lines',\n",
    "            name='Grid',  # 给网格设置名字，用于筛选\n",
    "            visible=True  # 默认显示\n",
    "        ))\n",
    "\n",
    "# 创建一个调色板来表示 category_id 的颜色\n",
    "category_colors = pc.qualitative.Set1\n",
    "marker_colors = [category_colors[c % len(category_colors)] for c in venue_category.cpu().numpy()]\n",
    "\n",
    "# 添加数据点图层\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=x.cpu().numpy(),\n",
    "    y=y.cpu().numpy(),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=marker_colors,  # 使用 category_id 作为颜色\n",
    "        showscale=True  # 显示颜色条\n",
    "    ),\n",
    "    name='Data Points',  # 给数据点设置名字，用于筛选\n",
    "    visible=True  # 默认显示\n",
    "))\n",
    "\n",
    "# 设置布局\n",
    "fig.update_layout(\n",
    "    title=\"Venue Points with Density-based Coloring and Grid Lines\",\n",
    "    xaxis_title=\"Longitude\",\n",
    "    yaxis_title=\"Latitude\",\n",
    "    showlegend=True,\n",
    "    updatemenus=[\n",
    "        {\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'args': [None, {'visible': [True, False]}],  # 隐藏数据点，只显示网格\n",
    "                    'label': 'Show Grid Only',\n",
    "                    'method': 'relayout'\n",
    "                },\n",
    "                {\n",
    "                    'args': [None, {'visible': [False, True]}],  # 隐藏网格，只显示数据点\n",
    "                    'label': 'Show Data Points Only',\n",
    "                    'method': 'relayout'\n",
    "                },\n",
    "                {\n",
    "                    'args': [None, {'visible': [True, True]}],  # 显示所有\n",
    "                    'label': 'Show Both',\n",
    "                    'method': 'relayout'\n",
    "                }\n",
    "            ],\n",
    "            'direction': 'down',\n",
    "            'showactive': True,\n",
    "            'x': 0.17,\n",
    "            'xanchor': 'left',\n",
    "            'y': 1.15,\n",
    "            'yanchor': 'top'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 在浏览器中显示\n",
    "fig.show(renderer=\"browser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地点：偏远中心稀有丰富\n",
    "街区：偏远中心密集稀疏\n",
    "用500m*500m作为基本地理块进行聚合\n",
    "街区密度阈值 街区偏远度阈值\n",
    "中心和密集相关\n",
    "丰富地点一定是便利店之类的常用地点\n",
    "重点是稀有地点区分\n",
    "偏远但密集的是乡镇 偏远密集街区的丰富地点是便民设施 稀有地点大概率是学校 政府等大场地 小概率是景点\n",
    "偏远稀疏的是无人区 偏远稀疏街区的丰富地点可能就是便利店 但是稀有地点很可能是景点\n",
    "中心稀疏是城郊 景点的概率较大\n",
    "中心密集的是市中心 这是推荐的重点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "def plot_and_upload_radar(metrics, run):\n",
    "    \"\"\"\n",
    "    根据测试指标绘制雷达图并上传到当前 wandb run。\n",
    "    \n",
    "    参数：\n",
    "    - metrics: dict, 包含最后测试的指标，格式如下：\n",
    "        {\n",
    "            \"MRR@5\": 0.8, \"MRR@10\": 0.85,\n",
    "            \"NDCG@5\": 0.78, \"NDCG@10\": 0.82,\n",
    "            \"Recall@5\": 0.75, \"Recall@10\": 0.8\n",
    "        }\n",
    "    - run: 当前 wandb.run 对象\n",
    "    \"\"\"\n",
    "    # 提取指标和标签\n",
    "    labels = [\"MRR@5\", \"MRR@10\", \"NDCG@5\", \"NDCG@10\", \"Recall@5\", \"Recall@10\"]\n",
    "    values = [metrics[label] for label in labels]\n",
    "    \n",
    "    # 将雷达图闭合\n",
    "    values += values[:1]\n",
    "    angles = np.linspace(0, 2 * np.pi, len(labels) + 1, endpoint=True)\n",
    "\n",
    "    # 创建雷达图\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw={'projection': 'polar'})\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label='Model Performance')\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "    \n",
    "    # 设置雷达图的标签\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=10)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, fontsize=10)\n",
    "\n",
    "    # 图例和标题\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "    ax.set_title(\"Radar Chart of Metrics\", fontsize=14)\n",
    "\n",
    "    # 保存图片\n",
    "    radar_path = \"radar_chart.png\"\n",
    "    plt.savefig(radar_path)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 上传到 wandb\n",
    "    run.log({\"Radar Chart\": wandb.Image(radar_path)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba4Rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from Modules.mamba4rec import Mamba4Rec\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=Mamba4Rec, config_file_list=['Configs/Mamba4rec.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.mamba4rec import Mamba4Rec\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=Mamba4Rec, config_file_list=['Configs/Mamba4rec.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = Mamba4Rec(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba4POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from Modules.mamba4poi import Mamba4POI\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=Mamba4POI, config_file_list=['Configs/Mamba4POI.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22 Dec 17:48    INFO  ['/home/chillypepper/anaconda3/envs/mamba4rec/lib/python3.8/site-packages/ipykernel_launcher.py', '--f=/home/chillypepper/.local/share/jupyter/runtime/kernel-v342e2448369c5d499bdac3e15279de89b3dd48572.json']\n",
      "22 Dec 17:48    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = dataset/foursquare_NYC\n",
      "checkpoint_dir = SavedData\n",
      "show_progress = True\n",
      "save_dataset = True\n",
      "dataset_save_path = SavedData/foursquare_NYC-FourSquare.pth\n",
      "save_dataloaders = True\n",
      "dataloaders_save_path = \n",
      "log_wandb = True\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 100\n",
      "train_batch_size = 1024\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 4\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}, 'spilt': {'RS': [0.7, 0.1, 0.2]}, 'gourp_by': 'user_id', 'max_seq_len': 128}\n",
      "repeatable = True\n",
      "metrics = ['Recall', 'MRR', 'NDCG']\n",
      "topk = [1, 5, 10, 20]\n",
      "valid_metric = NDCG@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 1024\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = venue_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'venue_id', 'timestamp'], 'item': ['venue_id', 'latitude', 'longitude', 'venue_category_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = venue_seq_len\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 128\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = Mamba4POI\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.SEQUENTIAL\n",
      "wandb_entity = KactusJec\n",
      "wandb_run_name = Mamba4POI-INdayTE\n",
      "logging = {'level': 'DEBUG'}\n",
      "hidden_size = 128\n",
      "num_layers = 1\n",
      "dropout_prob = 0.2\n",
      "loss_type = CE\n",
      "d_state = 32\n",
      "d_conv = 4\n",
      "expand = 2\n",
      "Use_CustomDataset = True\n",
      "Use_CustomSampler = False\n",
      "single_spec = True\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "local_rank = 0\n",
      "device = cuda\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "22 Dec 17:48    INFO  foursquare_NYC\n",
      "The number of users: 1084\n",
      "Average actions of users: 208.99815327793166\n",
      "The number of items: 38334\n",
      "Average actions of items: 5.916897579338109\n",
      "The number of inters: 226345\n",
      "The sparsity of the dataset: 99.45529986290629%\n",
      "Remain Fields: ['user_id', 'venue_id', 'timestamp', 'venue_category_id', 'latitude', 'longitude', 'venue_id_list', 'timestamp_list', 'venue_seq_len']\n",
      "22 Dec 17:48    INFO  Mamba4POI(\n",
      "  (itemloc_embedding): Embedding(38334, 64)\n",
      "  (itemcat_embedding): Embedding(38334, 64)\n",
      "  (userloc_embedding): Embedding(1084, 64)\n",
      "  (usercat_embedding): Embedding(1084, 64)\n",
      "  (itembase_embedding): Embedding(38334, 64, padding_idx=0)\n",
      "  (userbase_embedding): Embedding(1084, 64, padding_idx=0)\n",
      "  (Norm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (LocMamba): ModuleList(\n",
      "    (0): MambaLayer(\n",
      "      (mamba): Mamba(\n",
      "        (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "        (act): SiLU()\n",
      "        (x_proj): Linear(in_features=128, out_features=68, bias=False)\n",
      "        (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (w_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (w_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (CatMamba): ModuleList(\n",
      "    (0): MambaLayer(\n",
      "      (mamba): Mamba(\n",
      "        (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "        (act): SiLU()\n",
      "        (x_proj): Linear(in_features=128, out_features=68, bias=False)\n",
      "        (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (w_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (w_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_fct): CrossEntropyLoss()\n",
      ")\n",
      "Trainable parameters: 5189888\n",
      "22 Dec 17:48    INFO  FLOPs: 4292736.0\n",
      "22 Dec 17:48    ERROR  Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: kactusjec (kactusjec-hangzhou-dianzi-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/nvme0n1p2/Files/Code/Mamba/Mamba4POI/wandb/run-20241222_174850-1cd5z2s5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kactusjec-hangzhou-dianzi-university/Mamba4POI/runs/1cd5z2s5' target=\"_blank\">Mamba4POI-INdayTE</a></strong> to <a href='https://wandb.ai/kactusjec-hangzhou-dianzi-university/Mamba4POI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kactusjec-hangzhou-dianzi-university/Mamba4POI' target=\"_blank\">https://wandb.ai/kactusjec-hangzhou-dianzi-university/Mamba4POI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kactusjec-hangzhou-dianzi-university/Mamba4POI/runs/1cd5z2s5' target=\"_blank\">https://wandb.ai/kactusjec-hangzhou-dianzi-university/Mamba4POI/runs/1cd5z2s5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22 Dec 17:49    INFO  epoch 0 training [time: 50.51s, train loss: 4094.6556]\n",
      "22 Dec 17:49    INFO  epoch 0 evaluating [time: 2.31s, valid_score: 0.020600]\n",
      "22 Dec 17:49    INFO  valid result: \n",
      "recall@1 : 0.0109    recall@5 : 0.0253    recall@10 : 0.0328    recall@20 : 0.0434    mrr@1 : 0.0109    mrr@5 : 0.0159    mrr@10 : 0.0169    mrr@20 : 0.0176    ndcg@1 : 0.0109    ndcg@5 : 0.0182    ndcg@10 : 0.0206    ndcg@20 : 0.0233\n",
      "22 Dec 17:49    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 17:50    INFO  epoch 1 training [time: 53.93s, train loss: 2305.5733]\n",
      "22 Dec 17:50    INFO  epoch 1 evaluating [time: 2.65s, valid_score: 0.087700]\n",
      "22 Dec 17:50    INFO  valid result: \n",
      "recall@1 : 0.0559    recall@5 : 0.1026    recall@10 : 0.1246    recall@20 : 0.1501    mrr@1 : 0.0559    mrr@5 : 0.0732    mrr@10 : 0.0762    mrr@20 : 0.078    ndcg@1 : 0.0559    ndcg@5 : 0.0805    ndcg@10 : 0.0877    ndcg@20 : 0.0941\n",
      "22 Dec 17:50    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 17:51    INFO  epoch 2 training [time: 72.52s, train loss: 1731.1854]\n",
      "22 Dec 17:52    INFO  epoch 2 evaluating [time: 3.66s, valid_score: 0.135900]\n",
      "22 Dec 17:52    INFO  valid result: \n",
      "recall@1 : 0.0879    recall@5 : 0.1593    recall@10 : 0.1904    recall@20 : 0.222    mrr@1 : 0.0879    mrr@5 : 0.1148    mrr@10 : 0.1189    mrr@20 : 0.121    ndcg@1 : 0.0879    ndcg@5 : 0.1259    ndcg@10 : 0.1359    ndcg@20 : 0.1439\n",
      "22 Dec 17:52    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 17:53    INFO  epoch 3 training [time: 91.74s, train loss: 1524.5492]\n",
      "22 Dec 17:53    INFO  epoch 3 evaluating [time: 4.10s, valid_score: 0.165300]\n",
      "22 Dec 17:53    INFO  valid result: \n",
      "recall@1 : 0.1085    recall@5 : 0.1939    recall@10 : 0.2272    recall@20 : 0.2635    mrr@1 : 0.1085    mrr@5 : 0.1413    mrr@10 : 0.1458    mrr@20 : 0.1483    ndcg@1 : 0.1085    ndcg@5 : 0.1544    ndcg@10 : 0.1653    ndcg@20 : 0.1744\n",
      "22 Dec 17:53    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 17:55    INFO  epoch 4 training [time: 97.35s, train loss: 1454.6681]\n",
      "22 Dec 17:55    INFO  epoch 4 evaluating [time: 4.21s, valid_score: 0.181000]\n",
      "22 Dec 17:55    INFO  valid result: \n",
      "recall@1 : 0.1156    recall@5 : 0.2156    recall@10 : 0.2513    recall@20 : 0.2873    mrr@1 : 0.1156    mrr@5 : 0.154    mrr@10 : 0.1588    mrr@20 : 0.1613    ndcg@1 : 0.1156    ndcg@5 : 0.1694    ndcg@10 : 0.181    ndcg@20 : 0.1901\n",
      "22 Dec 17:55    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 17:57    INFO  epoch 5 training [time: 101.60s, train loss: 1409.4705]\n",
      "22 Dec 17:57    INFO  epoch 5 evaluating [time: 4.32s, valid_score: 0.194100]\n",
      "22 Dec 17:57    INFO  valid result: \n",
      "recall@1 : 0.1205    recall@5 : 0.2331    recall@10 : 0.272    recall@20 : 0.3092    mrr@1 : 0.1205    mrr@5 : 0.1642    mrr@10 : 0.1694    mrr@20 : 0.172    ndcg@1 : 0.1205    ndcg@5 : 0.1814    ndcg@10 : 0.1941    ndcg@20 : 0.2035\n",
      "22 Dec 17:57    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 17:58    INFO  epoch 6 training [time: 103.96s, train loss: 1369.9627]\n",
      "22 Dec 17:58    INFO  epoch 6 evaluating [time: 4.60s, valid_score: 0.203100]\n",
      "22 Dec 17:58    INFO  valid result: \n",
      "recall@1 : 0.1252    recall@5 : 0.2458    recall@10 : 0.2861    recall@20 : 0.3261    mrr@1 : 0.1252    mrr@5 : 0.1714    mrr@10 : 0.1768    mrr@20 : 0.1796    ndcg@1 : 0.1252    ndcg@5 : 0.19    ndcg@10 : 0.2031    ndcg@20 : 0.2132\n",
      "22 Dec 17:58    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:00    INFO  epoch 7 training [time: 105.93s, train loss: 1335.9392]\n",
      "22 Dec 18:00    INFO  epoch 7 evaluating [time: 4.33s, valid_score: 0.210100]\n",
      "22 Dec 18:00    INFO  valid result: \n",
      "recall@1 : 0.1265    recall@5 : 0.2568    recall@10 : 0.2991    recall@20 : 0.3376    mrr@1 : 0.1265    mrr@5 : 0.1762    mrr@10 : 0.1819    mrr@20 : 0.1846    ndcg@1 : 0.1265    ndcg@5 : 0.1964    ndcg@10 : 0.2101    ndcg@20 : 0.2199\n",
      "22 Dec 18:00    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:02    INFO  epoch 8 training [time: 104.89s, train loss: 1306.1388]\n",
      "22 Dec 18:02    INFO  epoch 8 evaluating [time: 5.73s, valid_score: 0.218100]\n",
      "22 Dec 18:02    INFO  valid result: \n",
      "recall@1 : 0.1316    recall@5 : 0.2667    recall@10 : 0.3094    recall@20 : 0.3497    mrr@1 : 0.1316    mrr@5 : 0.1834    mrr@10 : 0.1892    mrr@20 : 0.192    ndcg@1 : 0.1316    ndcg@5 : 0.2042    ndcg@10 : 0.2181    ndcg@20 : 0.2283\n",
      "22 Dec 18:02    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:04    INFO  epoch 9 training [time: 106.11s, train loss: 1278.5375]\n",
      "22 Dec 18:04    INFO  epoch 9 evaluating [time: 5.86s, valid_score: 0.224700]\n",
      "22 Dec 18:04    INFO  valid result: \n",
      "recall@1 : 0.134    recall@5 : 0.2747    recall@10 : 0.3217    recall@20 : 0.36    mrr@1 : 0.134    mrr@5 : 0.1877    mrr@10 : 0.194    mrr@20 : 0.1967    ndcg@1 : 0.134    ndcg@5 : 0.2095    ndcg@10 : 0.2247    ndcg@20 : 0.2344\n",
      "22 Dec 18:04    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:06    INFO  epoch 10 training [time: 107.45s, train loss: 1255.2069]\n",
      "22 Dec 18:06    INFO  epoch 10 evaluating [time: 5.81s, valid_score: 0.229100]\n",
      "22 Dec 18:06    INFO  valid result: \n",
      "recall@1 : 0.1357    recall@5 : 0.282    recall@10 : 0.3285    recall@20 : 0.3702    mrr@1 : 0.1357    mrr@5 : 0.1913    mrr@10 : 0.1976    mrr@20 : 0.2005    ndcg@1 : 0.1357    ndcg@5 : 0.214    ndcg@10 : 0.2291    ndcg@20 : 0.2397\n",
      "22 Dec 18:06    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:08    INFO  epoch 11 training [time: 107.49s, train loss: 1233.6978]\n",
      "22 Dec 18:08    INFO  epoch 11 evaluating [time: 3.38s, valid_score: 0.234200]\n",
      "22 Dec 18:08    INFO  valid result: \n",
      "recall@1 : 0.1382    recall@5 : 0.2882    recall@10 : 0.3363    recall@20 : 0.3784    mrr@1 : 0.1382    mrr@5 : 0.1953    mrr@10 : 0.2019    mrr@20 : 0.2048    ndcg@1 : 0.1382    ndcg@5 : 0.2186    ndcg@10 : 0.2342    ndcg@20 : 0.2449\n",
      "22 Dec 18:08    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:09    INFO  epoch 12 training [time: 89.93s, train loss: 1213.9228]\n",
      "22 Dec 18:09    INFO  epoch 12 evaluating [time: 4.08s, valid_score: 0.237900]\n",
      "22 Dec 18:09    INFO  valid result: \n",
      "recall@1 : 0.1397    recall@5 : 0.2935    recall@10 : 0.3426    recall@20 : 0.3861    mrr@1 : 0.1397    mrr@5 : 0.1981    mrr@10 : 0.2047    mrr@20 : 0.2078    ndcg@1 : 0.1397    ndcg@5 : 0.222    ndcg@10 : 0.2379    ndcg@20 : 0.2489\n",
      "22 Dec 18:09    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:11    INFO  epoch 13 training [time: 86.09s, train loss: 1195.2193]\n",
      "22 Dec 18:11    INFO  epoch 13 evaluating [time: 3.83s, valid_score: 0.241700]\n",
      "22 Dec 18:11    INFO  valid result: \n",
      "recall@1 : 0.1405    recall@5 : 0.2978    recall@10 : 0.349    recall@20 : 0.3953    mrr@1 : 0.1405    mrr@5 : 0.2008    mrr@10 : 0.2077    mrr@20 : 0.211    ndcg@1 : 0.1405    ndcg@5 : 0.2251    ndcg@10 : 0.2417    ndcg@20 : 0.2535\n",
      "22 Dec 18:11    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:12    INFO  epoch 14 training [time: 87.24s, train loss: 1178.0221]\n",
      "22 Dec 18:12    INFO  epoch 14 evaluating [time: 3.88s, valid_score: 0.246600]\n",
      "22 Dec 18:12    INFO  valid result: \n",
      "recall@1 : 0.143    recall@5 : 0.3035    recall@10 : 0.3569    recall@20 : 0.4024    mrr@1 : 0.143    mrr@5 : 0.2045    mrr@10 : 0.2117    mrr@20 : 0.2149    ndcg@1 : 0.143    ndcg@5 : 0.2293    ndcg@10 : 0.2466    ndcg@20 : 0.2581\n",
      "22 Dec 18:12    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:14    INFO  epoch 15 training [time: 85.00s, train loss: 1160.7196]\n",
      "22 Dec 18:14    INFO  epoch 15 evaluating [time: 3.25s, valid_score: 0.251000]\n",
      "22 Dec 18:14    INFO  valid result: \n",
      "recall@1 : 0.1467    recall@5 : 0.3082    recall@10 : 0.3625    recall@20 : 0.4084    mrr@1 : 0.1467    mrr@5 : 0.2083    mrr@10 : 0.2156    mrr@20 : 0.2189    ndcg@1 : 0.1467    ndcg@5 : 0.2333    ndcg@10 : 0.251    ndcg@20 : 0.2626\n",
      "22 Dec 18:14    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:15    INFO  epoch 16 training [time: 83.39s, train loss: 1144.9262]\n",
      "22 Dec 18:15    INFO  epoch 16 evaluating [time: 4.41s, valid_score: 0.253200]\n",
      "22 Dec 18:15    INFO  valid result: \n",
      "recall@1 : 0.1471    recall@5 : 0.3121    recall@10 : 0.3681    recall@20 : 0.4115    mrr@1 : 0.1471    mrr@5 : 0.2093    mrr@10 : 0.2169    mrr@20 : 0.2199    ndcg@1 : 0.1471    ndcg@5 : 0.235    ndcg@10 : 0.2532    ndcg@20 : 0.2642\n",
      "22 Dec 18:15    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:17    INFO  epoch 17 training [time: 83.64s, train loss: 1130.7751]\n",
      "22 Dec 18:17    INFO  epoch 17 evaluating [time: 3.92s, valid_score: 0.255900]\n",
      "22 Dec 18:17    INFO  valid result: \n",
      "recall@1 : 0.1476    recall@5 : 0.3161    recall@10 : 0.3729    recall@20 : 0.4193    mrr@1 : 0.1476    mrr@5 : 0.2113    mrr@10 : 0.2189    mrr@20 : 0.2222    ndcg@1 : 0.1476    ndcg@5 : 0.2375    ndcg@10 : 0.2559    ndcg@20 : 0.2677\n",
      "22 Dec 18:17    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:18    INFO  epoch 18 training [time: 83.10s, train loss: 1116.3008]\n",
      "22 Dec 18:18    INFO  epoch 18 evaluating [time: 3.64s, valid_score: 0.259100]\n",
      "22 Dec 18:18    INFO  valid result: \n",
      "recall@1 : 0.1494    recall@5 : 0.3205    recall@10 : 0.3763    recall@20 : 0.425    mrr@1 : 0.1494    mrr@5 : 0.2143    mrr@10 : 0.2219    mrr@20 : 0.2253    ndcg@1 : 0.1494    ndcg@5 : 0.2408    ndcg@10 : 0.2591    ndcg@20 : 0.2714\n",
      "22 Dec 18:18    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:20    INFO  epoch 19 training [time: 83.23s, train loss: 1102.8743]\n",
      "22 Dec 18:20    INFO  epoch 19 evaluating [time: 2.72s, valid_score: 0.261800]\n",
      "22 Dec 18:20    INFO  valid result: \n",
      "recall@1 : 0.1505    recall@5 : 0.3238    recall@10 : 0.3824    recall@20 : 0.4285    mrr@1 : 0.1505    mrr@5 : 0.2157    mrr@10 : 0.2237    mrr@20 : 0.2269    ndcg@1 : 0.1505    ndcg@5 : 0.2427    ndcg@10 : 0.2618    ndcg@20 : 0.2734\n",
      "22 Dec 18:20    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:21    INFO  epoch 20 training [time: 81.74s, train loss: 1089.2702]\n",
      "22 Dec 18:21    INFO  epoch 20 evaluating [time: 3.93s, valid_score: 0.264400]\n",
      "22 Dec 18:21    INFO  valid result: \n",
      "recall@1 : 0.1522    recall@5 : 0.3262    recall@10 : 0.3859    recall@20 : 0.4321    mrr@1 : 0.1522    mrr@5 : 0.218    mrr@10 : 0.226    mrr@20 : 0.2293    ndcg@1 : 0.1522    ndcg@5 : 0.245    ndcg@10 : 0.2644    ndcg@20 : 0.2761\n",
      "22 Dec 18:21    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:22    INFO  epoch 21 training [time: 81.36s, train loss: 1075.9695]\n",
      "22 Dec 18:22    INFO  epoch 21 evaluating [time: 3.88s, valid_score: 0.265900]\n",
      "22 Dec 18:22    INFO  valid result: \n",
      "recall@1 : 0.1529    recall@5 : 0.3267    recall@10 : 0.3887    recall@20 : 0.434    mrr@1 : 0.1529    mrr@5 : 0.2188    mrr@10 : 0.2271    mrr@20 : 0.2303    ndcg@1 : 0.1529    ndcg@5 : 0.2458    ndcg@10 : 0.2659    ndcg@20 : 0.2774\n",
      "22 Dec 18:22    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:24    INFO  epoch 22 training [time: 81.09s, train loss: 1064.5871]\n",
      "22 Dec 18:24    INFO  epoch 22 evaluating [time: 3.84s, valid_score: 0.267400]\n",
      "22 Dec 18:24    INFO  valid result: \n",
      "recall@1 : 0.1522    recall@5 : 0.3313    recall@10 : 0.3915    recall@20 : 0.4385    mrr@1 : 0.1522    mrr@5 : 0.22    mrr@10 : 0.2282    mrr@20 : 0.2315    ndcg@1 : 0.1522    ndcg@5 : 0.2478    ndcg@10 : 0.2674    ndcg@20 : 0.2793\n",
      "22 Dec 18:24    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:25    INFO  epoch 23 training [time: 80.68s, train loss: 1052.3850]\n",
      "22 Dec 18:25    INFO  epoch 23 evaluating [time: 3.53s, valid_score: 0.269200]\n",
      "22 Dec 18:25    INFO  valid result: \n",
      "recall@1 : 0.154    recall@5 : 0.3321    recall@10 : 0.394    recall@20 : 0.4434    mrr@1 : 0.154    mrr@5 : 0.2214    mrr@10 : 0.2298    mrr@20 : 0.2333    ndcg@1 : 0.154    ndcg@5 : 0.2491    ndcg@10 : 0.2692    ndcg@20 : 0.2818\n",
      "22 Dec 18:25    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:27    INFO  epoch 24 training [time: 82.42s, train loss: 1040.4658]\n",
      "22 Dec 18:27    INFO  epoch 24 evaluating [time: 1.99s, valid_score: 0.269700]\n",
      "22 Dec 18:27    INFO  valid result: \n",
      "recall@1 : 0.153    recall@5 : 0.3336    recall@10 : 0.3969    recall@20 : 0.4466    mrr@1 : 0.153    mrr@5 : 0.221    mrr@10 : 0.2295    mrr@20 : 0.233    ndcg@1 : 0.153    ndcg@5 : 0.2491    ndcg@10 : 0.2697    ndcg@20 : 0.2823\n",
      "22 Dec 18:27    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:28    INFO  epoch 25 training [time: 83.35s, train loss: 1029.3262]\n",
      "22 Dec 18:28    INFO  epoch 25 evaluating [time: 3.00s, valid_score: 0.272300]\n",
      "22 Dec 18:28    INFO  valid result: \n",
      "recall@1 : 0.1556    recall@5 : 0.3364    recall@10 : 0.3992    recall@20 : 0.4496    mrr@1 : 0.1556    mrr@5 : 0.2238    mrr@10 : 0.2323    mrr@20 : 0.2358    ndcg@1 : 0.1556    ndcg@5 : 0.2519    ndcg@10 : 0.2723    ndcg@20 : 0.2851\n",
      "22 Dec 18:28    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:29    INFO  epoch 26 training [time: 81.48s, train loss: 1018.5171]\n",
      "22 Dec 18:30    INFO  epoch 26 evaluating [time: 2.17s, valid_score: 0.273500]\n",
      "22 Dec 18:30    INFO  valid result: \n",
      "recall@1 : 0.1568    recall@5 : 0.3373    recall@10 : 0.4005    recall@20 : 0.4538    mrr@1 : 0.1568    mrr@5 : 0.2247    mrr@10 : 0.2334    mrr@20 : 0.2371    ndcg@1 : 0.1568    ndcg@5 : 0.2529    ndcg@10 : 0.2735    ndcg@20 : 0.287\n",
      "22 Dec 18:30    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:31    INFO  epoch 27 training [time: 79.21s, train loss: 1007.1000]\n",
      "22 Dec 18:31    INFO  epoch 27 evaluating [time: 3.20s, valid_score: 0.274500]\n",
      "22 Dec 18:31    INFO  valid result: \n",
      "recall@1 : 0.1555    recall@5 : 0.3407    recall@10 : 0.405    recall@20 : 0.4551    mrr@1 : 0.1555    mrr@5 : 0.2247    mrr@10 : 0.2334    mrr@20 : 0.2369    ndcg@1 : 0.1555    ndcg@5 : 0.2536    ndcg@10 : 0.2745    ndcg@20 : 0.2873\n",
      "22 Dec 18:31    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:32    INFO  epoch 28 training [time: 80.81s, train loss: 996.8912]\n",
      "22 Dec 18:32    INFO  epoch 28 evaluating [time: 3.62s, valid_score: 0.274600]\n",
      "22 Dec 18:32    INFO  valid result: \n",
      "recall@1 : 0.1546    recall@5 : 0.3407    recall@10 : 0.4054    recall@20 : 0.4595    mrr@1 : 0.1546    mrr@5 : 0.2245    mrr@10 : 0.2333    mrr@20 : 0.2371    ndcg@1 : 0.1546    ndcg@5 : 0.2536    ndcg@10 : 0.2746    ndcg@20 : 0.2883\n",
      "22 Dec 18:32    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:34    INFO  epoch 29 training [time: 84.37s, train loss: 987.0307]\n",
      "22 Dec 18:34    INFO  epoch 29 evaluating [time: 3.86s, valid_score: 0.277600]\n",
      "22 Dec 18:34    INFO  valid result: \n",
      "recall@1 : 0.1601    recall@5 : 0.3406    recall@10 : 0.4077    recall@20 : 0.4599    mrr@1 : 0.1601    mrr@5 : 0.2276    mrr@10 : 0.2367    mrr@20 : 0.2404    ndcg@1 : 0.1601    ndcg@5 : 0.2558    ndcg@10 : 0.2776    ndcg@20 : 0.2909\n",
      "22 Dec 18:34    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:35    INFO  epoch 30 training [time: 80.03s, train loss: 976.4654]\n",
      "22 Dec 18:35    INFO  epoch 30 evaluating [time: 3.37s, valid_score: 0.278700]\n",
      "22 Dec 18:35    INFO  valid result: \n",
      "recall@1 : 0.1596    recall@5 : 0.3441    recall@10 : 0.4091    recall@20 : 0.4618    mrr@1 : 0.1596    mrr@5 : 0.2287    mrr@10 : 0.2376    mrr@20 : 0.2412    ndcg@1 : 0.1596    ndcg@5 : 0.2575    ndcg@10 : 0.2787    ndcg@20 : 0.292\n",
      "22 Dec 18:35    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:37    INFO  epoch 31 training [time: 83.49s, train loss: 967.5334]\n",
      "22 Dec 18:37    INFO  epoch 31 evaluating [time: 2.87s, valid_score: 0.278500]\n",
      "22 Dec 18:37    INFO  valid result: \n",
      "recall@1 : 0.1583    recall@5 : 0.3444    recall@10 : 0.4113    recall@20 : 0.4656    mrr@1 : 0.1583    mrr@5 : 0.2277    mrr@10 : 0.2367    mrr@20 : 0.2405    ndcg@1 : 0.1583    ndcg@5 : 0.2568    ndcg@10 : 0.2785    ndcg@20 : 0.2923\n",
      "22 Dec 18:38    INFO  epoch 32 training [time: 81.66s, train loss: 957.3235]\n",
      "22 Dec 18:38    INFO  epoch 32 evaluating [time: 3.90s, valid_score: 0.278500]\n",
      "22 Dec 18:38    INFO  valid result: \n",
      "recall@1 : 0.1582    recall@5 : 0.3444    recall@10 : 0.4109    recall@20 : 0.4656    mrr@1 : 0.1582    mrr@5 : 0.2277    mrr@10 : 0.2367    mrr@20 : 0.2405    ndcg@1 : 0.1582    ndcg@5 : 0.2568    ndcg@10 : 0.2785    ndcg@20 : 0.2923\n",
      "22 Dec 18:40    INFO  epoch 33 training [time: 98.58s, train loss: 948.9171]\n",
      "22 Dec 18:40    INFO  epoch 33 evaluating [time: 5.36s, valid_score: 0.279500]\n",
      "22 Dec 18:40    INFO  valid result: \n",
      "recall@1 : 0.1592    recall@5 : 0.3449    recall@10 : 0.4127    recall@20 : 0.4658    mrr@1 : 0.1592    mrr@5 : 0.2284    mrr@10 : 0.2376    mrr@20 : 0.2413    ndcg@1 : 0.1592    ndcg@5 : 0.2574    ndcg@10 : 0.2795    ndcg@20 : 0.293\n",
      "22 Dec 18:40    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:41    INFO  epoch 34 training [time: 102.64s, train loss: 939.3211]\n",
      "22 Dec 18:42    INFO  epoch 34 evaluating [time: 3.77s, valid_score: 0.279100]\n",
      "22 Dec 18:42    INFO  valid result: \n",
      "recall@1 : 0.1583    recall@5 : 0.3443    recall@10 : 0.4132    recall@20 : 0.4668    mrr@1 : 0.1583    mrr@5 : 0.2278    mrr@10 : 0.237    mrr@20 : 0.2408    ndcg@1 : 0.1583    ndcg@5 : 0.2569    ndcg@10 : 0.2791    ndcg@20 : 0.2927\n",
      "22 Dec 18:43    INFO  epoch 35 training [time: 102.57s, train loss: 930.7293]\n",
      "22 Dec 18:43    INFO  epoch 35 evaluating [time: 3.28s, valid_score: 0.279400]\n",
      "22 Dec 18:43    INFO  valid result: \n",
      "recall@1 : 0.1589    recall@5 : 0.343    recall@10 : 0.4134    recall@20 : 0.4668    mrr@1 : 0.1589    mrr@5 : 0.2278    mrr@10 : 0.2373    mrr@20 : 0.241    ndcg@1 : 0.1589    ndcg@5 : 0.2565    ndcg@10 : 0.2794    ndcg@20 : 0.2929\n",
      "22 Dec 18:45    INFO  epoch 36 training [time: 105.11s, train loss: 921.1428]\n",
      "22 Dec 18:45    INFO  epoch 36 evaluating [time: 4.88s, valid_score: 0.280000]\n",
      "22 Dec 18:45    INFO  valid result: \n",
      "recall@1 : 0.1596    recall@5 : 0.3443    recall@10 : 0.4142    recall@20 : 0.4694    mrr@1 : 0.1596    mrr@5 : 0.2284    mrr@10 : 0.2379    mrr@20 : 0.2417    ndcg@1 : 0.1596    ndcg@5 : 0.2573    ndcg@10 : 0.28    ndcg@20 : 0.294\n",
      "22 Dec 18:45    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:47    INFO  epoch 37 training [time: 101.51s, train loss: 913.0960]\n",
      "22 Dec 18:47    INFO  epoch 37 evaluating [time: 4.29s, valid_score: 0.281000]\n",
      "22 Dec 18:47    INFO  valid result: \n",
      "recall@1 : 0.1587    recall@5 : 0.3459    recall@10 : 0.4171    recall@20 : 0.4724    mrr@1 : 0.1587    mrr@5 : 0.2286    mrr@10 : 0.2382    mrr@20 : 0.2421    ndcg@1 : 0.1587    ndcg@5 : 0.2578    ndcg@10 : 0.281    ndcg@20 : 0.295\n",
      "22 Dec 18:47    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:48    INFO  epoch 38 training [time: 95.52s, train loss: 904.9080]\n",
      "22 Dec 18:49    INFO  epoch 38 evaluating [time: 3.41s, valid_score: 0.280200]\n",
      "22 Dec 18:49    INFO  valid result: \n",
      "recall@1 : 0.1589    recall@5 : 0.3437    recall@10 : 0.4152    recall@20 : 0.4706    mrr@1 : 0.1589    mrr@5 : 0.2281    mrr@10 : 0.2378    mrr@20 : 0.2417    ndcg@1 : 0.1589    ndcg@5 : 0.257    ndcg@10 : 0.2802    ndcg@20 : 0.2942\n",
      "22 Dec 18:50    INFO  epoch 39 training [time: 95.90s, train loss: 895.4233]\n",
      "22 Dec 18:50    INFO  epoch 39 evaluating [time: 5.60s, valid_score: 0.279400]\n",
      "22 Dec 18:50    INFO  valid result: \n",
      "recall@1 : 0.1591    recall@5 : 0.3457    recall@10 : 0.4124    recall@20 : 0.4692    mrr@1 : 0.1591    mrr@5 : 0.2285    mrr@10 : 0.2376    mrr@20 : 0.2416    ndcg@1 : 0.1591    ndcg@5 : 0.2577    ndcg@10 : 0.2794    ndcg@20 : 0.2939\n",
      "22 Dec 18:52    INFO  epoch 40 training [time: 96.88s, train loss: 887.8596]\n",
      "22 Dec 18:52    INFO  epoch 40 evaluating [time: 4.08s, valid_score: 0.280000]\n",
      "22 Dec 18:52    INFO  valid result: \n",
      "recall@1 : 0.1593    recall@5 : 0.3452    recall@10 : 0.4146    recall@20 : 0.4695    mrr@1 : 0.1593    mrr@5 : 0.2283    mrr@10 : 0.2377    mrr@20 : 0.2415    ndcg@1 : 0.1593    ndcg@5 : 0.2574    ndcg@10 : 0.28    ndcg@20 : 0.2939\n",
      "22 Dec 18:54    INFO  epoch 41 training [time: 99.36s, train loss: 880.0160]\n",
      "22 Dec 18:54    INFO  epoch 41 evaluating [time: 5.01s, valid_score: 0.280600]\n",
      "22 Dec 18:54    INFO  valid result: \n",
      "recall@1 : 0.1588    recall@5 : 0.3466    recall@10 : 0.4159    recall@20 : 0.4706    mrr@1 : 0.1588    mrr@5 : 0.2287    mrr@10 : 0.238    mrr@20 : 0.2419    ndcg@1 : 0.1588    ndcg@5 : 0.2581    ndcg@10 : 0.2806    ndcg@20 : 0.2945\n",
      "22 Dec 18:55    INFO  epoch 42 training [time: 95.60s, train loss: 871.6530]\n",
      "22 Dec 18:55    INFO  epoch 42 evaluating [time: 4.53s, valid_score: 0.281200]\n",
      "22 Dec 18:55    INFO  valid result: \n",
      "recall@1 : 0.1599    recall@5 : 0.3461    recall@10 : 0.4163    recall@20 : 0.472    mrr@1 : 0.1599    mrr@5 : 0.2292    mrr@10 : 0.2387    mrr@20 : 0.2426    ndcg@1 : 0.1599    ndcg@5 : 0.2584    ndcg@10 : 0.2812    ndcg@20 : 0.2953\n",
      "22 Dec 18:55    INFO  Saving current: SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "22 Dec 18:57    INFO  epoch 43 training [time: 98.41s, train loss: 864.1217]\n",
      "22 Dec 18:57    INFO  epoch 43 evaluating [time: 4.18s, valid_score: 0.280000]\n",
      "22 Dec 18:57    INFO  valid result: \n",
      "recall@1 : 0.1583    recall@5 : 0.3444    recall@10 : 0.4162    recall@20 : 0.4718    mrr@1 : 0.1583    mrr@5 : 0.2275    mrr@10 : 0.2372    mrr@20 : 0.2411    ndcg@1 : 0.1583    ndcg@5 : 0.2566    ndcg@10 : 0.28    ndcg@20 : 0.2941\n",
      "22 Dec 18:59    INFO  epoch 44 training [time: 101.90s, train loss: 856.5643]\n",
      "22 Dec 18:59    INFO  epoch 44 evaluating [time: 3.20s, valid_score: 0.280600]\n",
      "22 Dec 18:59    INFO  valid result: \n",
      "recall@1 : 0.1586    recall@5 : 0.3453    recall@10 : 0.4164    recall@20 : 0.4716    mrr@1 : 0.1586    mrr@5 : 0.2283    mrr@10 : 0.2379    mrr@20 : 0.2418    ndcg@1 : 0.1586    ndcg@5 : 0.2575    ndcg@10 : 0.2806    ndcg@20 : 0.2946\n",
      "22 Dec 19:00    INFO  epoch 45 training [time: 100.43s, train loss: 848.9091]\n",
      "22 Dec 19:01    INFO  epoch 45 evaluating [time: 3.48s, valid_score: 0.280500]\n",
      "22 Dec 19:01    INFO  valid result: \n",
      "recall@1 : 0.1598    recall@5 : 0.3452    recall@10 : 0.4153    recall@20 : 0.4711    mrr@1 : 0.1598    mrr@5 : 0.2286    mrr@10 : 0.2381    mrr@20 : 0.242    ndcg@1 : 0.1598    ndcg@5 : 0.2577    ndcg@10 : 0.2805    ndcg@20 : 0.2946\n",
      "22 Dec 19:02    INFO  epoch 46 training [time: 98.65s, train loss: 841.1449]\n",
      "22 Dec 19:02    INFO  epoch 46 evaluating [time: 3.75s, valid_score: 0.280700]\n",
      "22 Dec 19:02    INFO  valid result: \n",
      "recall@1 : 0.1587    recall@5 : 0.3466    recall@10 : 0.4165    recall@20 : 0.4728    mrr@1 : 0.1587    mrr@5 : 0.2286    mrr@10 : 0.238    mrr@20 : 0.2419    ndcg@1 : 0.1587    ndcg@5 : 0.258    ndcg@10 : 0.2807    ndcg@20 : 0.2949\n",
      "22 Dec 19:04    INFO  epoch 47 training [time: 97.91s, train loss: 833.8455]\n",
      "22 Dec 19:04    INFO  epoch 47 evaluating [time: 4.04s, valid_score: 0.277400]\n",
      "22 Dec 19:04    INFO  valid result: \n",
      "recall@1 : 0.1561    recall@5 : 0.3434    recall@10 : 0.4119    recall@20 : 0.4689    mrr@1 : 0.1561    mrr@5 : 0.2258    mrr@10 : 0.2351    mrr@20 : 0.2391    ndcg@1 : 0.1561    ndcg@5 : 0.2552    ndcg@10 : 0.2774    ndcg@20 : 0.2919\n",
      "22 Dec 19:04    INFO  Finished training, best eval result in epoch 42\n",
      "22 Dec 19:04    INFO  Loading model structure and parameters from SavedData/Mamba4POI-Dec-22-2024_17-48-57.pth\n",
      "Evaluate   :   0%|                                                           | 0/22 [00:00<?, ?it/s]:   0%|                                   | 0/22 [00:00<?, ?it/s, GPU RAM: 1.70 G/5.79 G]:   5%|█▏                         | 1/22 [00:00<00:03,  6.59it/s, GPU RAM: 1.70 G/5.79 G]:   5%|█▏                         | 1/22 [00:00<00:03,  6.59it/s, GPU RAM: 1.70 G/5.79 G]:   9%|██▍                        | 2/22 [00:00<00:03,  5.77it/s, GPU RAM: 1.70 G/5.79 G]:   9%|██▍                        | 2/22 [00:00<00:03,  5.77it/s, GPU RAM: 1.70 G/5.79 G]:  14%|███▋                       | 3/22 [00:00<00:03,  5.55it/s, GPU RAM: 1.70 G/5.79 G]:  14%|███▋                       | 3/22 [00:00<00:03,  5.55it/s, GPU RAM: 1.70 G/5.79 G]:  18%|████▉                      | 4/22 [00:00<00:03,  5.47it/s, GPU RAM: 1.70 G/5.79 G]:  18%|████▉                      | 4/22 [00:00<00:03,  5.47it/s, GPU RAM: 1.70 G/5.79 G]:  23%|██████▏                    | 5/22 [00:00<00:03,  5.37it/s, GPU RAM: 1.70 G/5.79 G]:  23%|██████▏                    | 5/22 [00:00<00:03,  5.37it/s, GPU RAM: 1.70 G/5.79 G]:  27%|███████▎                   | 6/22 [00:01<00:03,  5.28it/s, GPU RAM: 1.70 G/5.79 G]:  27%|███████▎                   | 6/22 [00:01<00:03,  5.28it/s, GPU RAM: 1.70 G/5.79 G]:  32%|████████▌                  | 7/22 [00:01<00:02,  5.23it/s, GPU RAM: 1.70 G/5.79 G]:  32%|████████▌                  | 7/22 [00:01<00:02,  5.23it/s, GPU RAM: 1.70 G/5.79 G]:  36%|█████████▊                 | 8/22 [00:01<00:02,  5.32it/s, GPU RAM: 1.70 G/5.79 G]:  36%|█████████▊                 | 8/22 [00:01<00:02,  5.32it/s, GPU RAM: 1.70 G/5.79 G]:  41%|███████████                | 9/22 [00:01<00:02,  5.34it/s, GPU RAM: 1.70 G/5.79 G]:  41%|███████████                | 9/22 [00:01<00:02,  5.34it/s, GPU RAM: 1.70 G/5.79 G]:  45%|███████████▊              | 10/22 [00:01<00:02,  5.26it/s, GPU RAM: 1.70 G/5.79 G]:  45%|███████████▊              | 10/22 [00:01<00:02,  5.26it/s, GPU RAM: 1.70 G/5.79 G]:  50%|█████████████             | 11/22 [00:02<00:02,  5.27it/s, GPU RAM: 1.70 G/5.79 G]:  50%|█████████████             | 11/22 [00:02<00:02,  5.27it/s, GPU RAM: 1.70 G/5.79 G]:  55%|██████████████▏           | 12/22 [00:02<00:01,  5.24it/s, GPU RAM: 1.70 G/5.79 G]:  55%|██████████████▏           | 12/22 [00:02<00:01,  5.24it/s, GPU RAM: 1.70 G/5.79 G]:  59%|███████████████▎          | 13/22 [00:02<00:01,  5.26it/s, GPU RAM: 1.70 G/5.79 G]:  59%|███████████████▎          | 13/22 [00:02<00:01,  5.26it/s, GPU RAM: 1.70 G/5.79 G]:  64%|████████████████▌         | 14/22 [00:02<00:01,  5.15it/s, GPU RAM: 1.70 G/5.79 G]:  64%|████████████████▌         | 14/22 [00:02<00:01,  5.15it/s, GPU RAM: 1.70 G/5.79 G]:  68%|█████████████████▋        | 15/22 [00:02<00:01,  5.18it/s, GPU RAM: 1.70 G/5.79 G]:  68%|█████████████████▋        | 15/22 [00:02<00:01,  5.18it/s, GPU RAM: 1.70 G/5.79 G]:  73%|██████████████████▉       | 16/22 [00:03<00:01,  5.13it/s, GPU RAM: 1.70 G/5.79 G]:  73%|██████████████████▉       | 16/22 [00:03<00:01,  5.13it/s, GPU RAM: 1.70 G/5.79 G]:  77%|████████████████████      | 17/22 [00:03<00:00,  5.20it/s, GPU RAM: 1.70 G/5.79 G]:  77%|████████████████████      | 17/22 [00:03<00:00,  5.20it/s, GPU RAM: 1.70 G/5.79 G]:  82%|█████████████████████▎    | 18/22 [00:03<00:00,  5.14it/s, GPU RAM: 1.70 G/5.79 G]:  82%|█████████████████████▎    | 18/22 [00:03<00:00,  5.14it/s, GPU RAM: 1.70 G/5.79 G]:  86%|██████████████████████▍   | 19/22 [00:03<00:00,  5.05it/s, GPU RAM: 1.70 G/5.79 G]:  86%|██████████████████████▍   | 19/22 [00:03<00:00,  5.05it/s, GPU RAM: 1.70 G/5.79 G]:  91%|███████████████████████▋  | 20/22 [00:03<00:00,  5.15it/s, GPU RAM: 1.70 G/5.79 G]:  91%|███████████████████████▋  | 20/22 [00:03<00:00,  5.15it/s, GPU RAM: 1.70 G/5.79 G]:  95%|████████████████████████▊ | 21/22 [00:03<00:00,  5.16it/s, GPU RAM: 1.70 G/5.79 G]:  95%|████████████████████████▊ | 21/22 [00:04<00:00,  5.16it/s, GPU RAM: 1.70 G/5.79 G]: 100%|██████████████████████████| 22/22 [00:04<00:00,  5.85it/s, GPU RAM: 1.70 G/5.79 G]: 100%|██████████████████████████| 22/22 [00:04<00:00,  5.35it/s, GPU RAM: 1.70 G/5.79 G]\n",
      "22 Dec 19:04    INFO  The running environment of this training is as follows:\n",
      "+-------------+----------------+\n",
      "| Environment |     Usage      |\n",
      "+=============+================+\n",
      "| CPU         |    11.90 %     |\n",
      "+-------------+----------------+\n",
      "| GPU         | 1.70 G/5.79 G  |\n",
      "+-------------+----------------+\n",
      "| Memory      | 2.20 G/15.46 G |\n",
      "+-------------+----------------+\n",
      "22 Dec 19:04    INFO  best valid : OrderedDict([('recall@1', 0.1599), ('recall@5', 0.3461), ('recall@10', 0.4163), ('recall@20', 0.472), ('mrr@1', 0.1599), ('mrr@5', 0.2292), ('mrr@10', 0.2387), ('mrr@20', 0.2426), ('ndcg@1', 0.1599), ('ndcg@5', 0.2584), ('ndcg@10', 0.2812), ('ndcg@20', 0.2953)])\n",
      "22 Dec 19:04    INFO  test result: OrderedDict([('recall@1', 0.158), ('recall@5', 0.3301), ('recall@10', 0.3943), ('recall@20', 0.445), ('mrr@1', 0.158), ('mrr@5', 0.2226), ('mrr@10', 0.2313), ('mrr@20', 0.2349), ('ndcg@1', 0.158), ('ndcg@5', 0.2494), ('ndcg@10', 0.2703), ('ndcg@20', 0.2833)])\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c41e0d0ba741089d5dea1b9d8ddb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "from Modules.mamba4poi import Mamba4POI\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=Mamba4POI, config_file_list=['Configs/Mamba4POI.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = Mamba4POI(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORESTMamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22 Dec 19:12    INFO  Load filtered dataset from: [SavedData/foursquare_NYC-FourSquare.pth]\n",
      "22 Dec 19:12    INFO  Load split dataloaders from: [SavedData/foursquare_NYC-for-Mamba4POI-dataloader.pth]\n",
      "22 Dec 19:13    INFO  [Training]: train_batch_size = [1024] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "22 Dec 19:13    INFO  [Evaluation]: eval_batch_size = [1024] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}, 'spilt': {'RS': [0.7, 0.1, 0.2]}, 'gourp_by': 'user_id', 'max_seq_len': 128}]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from Modules.mamba4poi import Mamba4POI\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=Mamba4POI, config_file_list=['Configs/Mamba4POI.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22 Dec 19:13    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = dataset/foursquare_NYC\n",
      "checkpoint_dir = SavedData\n",
      "show_progress = True\n",
      "save_dataset = True\n",
      "dataset_save_path = SavedData/foursquare_NYC-FourSquare.pth\n",
      "save_dataloaders = True\n",
      "dataloaders_save_path = \n",
      "log_wandb = True\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 40\n",
      "train_batch_size = 1024\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 4\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}, 'spilt': {'RS': [0.7, 0.1, 0.2]}, 'gourp_by': 'user_id', 'max_seq_len': 128}\n",
      "repeatable = True\n",
      "metrics = ['Recall', 'MRR', 'NDCG']\n",
      "topk = [1, 5, 10, 20]\n",
      "valid_metric = NDCG@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 1024\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = venue_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'venue_id', 'timestamp'], 'item': ['venue_id', 'latitude', 'longitude', 'venue_category_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = venue_seq_len\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 128\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = Mamba4POI\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "inner_size = 256\n",
      "n_layers = 2\n",
      "n_heads = 2\n",
      "hidden_dropout_prob = 0.5\n",
      "attn_dropout_prob = 0.5\n",
      "hidden_act = gelu\n",
      "layer_norm_eps = 1e-12\n",
      "initializer_range = 0.02\n",
      "loss_type = CE\n",
      "dnn_type = trm\n",
      "sess_dropout = 0.2\n",
      "item_dropout = 0.2\n",
      "temperature = 0.07\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.SEQUENTIAL\n",
      "wandb_entity = KactusJec\n",
      "wandb_run_name = CORErerun\n",
      "logging = {'level': 'DEBUG'}\n",
      "save_checkpoint = True\n",
      "save_best_checkpoint = True\n",
      "Use_CustomDataset = True\n",
      "Use_CustomSampler = False\n",
      "single_spec = True\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "local_rank = 0\n",
      "device = cuda\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "22 Dec 19:13    INFO  Load filtered dataset from: [SavedData/foursquare_NYC-FourSquare.pth]\n",
      "22 Dec 19:13    INFO  foursquare_NYC\n",
      "The number of users: 1084\n",
      "Average actions of users: 209.99815327793166\n",
      "The number of items: 38334\n",
      "Average actions of items: 5.932955938747294\n",
      "The number of inters: 227428\n",
      "The sparsity of the dataset: 99.45269361912588%\n",
      "Remain Fields: ['user_id', 'venue_id', 'timestamp', 'venue_category_id', 'latitude', 'longitude']\n",
      "22 Dec 19:13    INFO  Load split dataloaders from: [SavedData/foursquare_NYC-for-CORE-dataloader.pth]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7d2ccfa080a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chillypepper/anaconda3/envs/mamba4rec/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "22 Dec 19:13    INFO  [Training]: train_batch_size = [1024] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "22 Dec 19:13    INFO  [Evaluation]: eval_batch_size = [1024] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}, 'spilt': {'RS': [0.7, 0.1, 0.2]}, 'gourp_by': 'user_id', 'max_seq_len': 128}]\n",
      "22 Dec 19:13    INFO  ['/home/chillypepper/anaconda3/envs/mamba4rec/lib/python3.8/site-packages/ipykernel_launcher.py', '--f=/home/chillypepper/.local/share/jupyter/runtime/kernel-v342e2448369c5d499bdac3e15279de89b3dd48572.json']\n",
      "22 Dec 19:13    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = dataset/foursquare_NYC\n",
      "checkpoint_dir = SavedData\n",
      "show_progress = True\n",
      "save_dataset = True\n",
      "dataset_save_path = SavedData/foursquare_NYC-FourSquare.pth\n",
      "save_dataloaders = True\n",
      "dataloaders_save_path = SavedData/COREMambaBPR-DL.pth\n",
      "log_wandb = True\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 40\n",
      "train_batch_size = 1024\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 4\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}, 'spilt': {'RS': [0.7, 0.1, 0.2]}, 'gourp_by': 'user_id', 'max_seq_len': 128}\n",
      "repeatable = True\n",
      "metrics = ['Recall', 'MRR', 'NDCG']\n",
      "topk = [1, 5, 10, 20]\n",
      "valid_metric = NDCG@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 1024\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = venue_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'venue_id', 'timestamp'], 'item': ['venue_id', 'latitude', 'longitude', 'venue_category_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = venue_seq_len\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 128\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = Mamba4POI\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.SEQUENTIAL\n",
      "wandb_entity = KactusJec\n",
      "wandb_run_name = COREMamba-UN-RL\n",
      "logging = {'level': 'DEBUG'}\n",
      "hidden_size = 128\n",
      "num_layers = 1\n",
      "dropout_prob = 0.2\n",
      "loss_type = CE\n",
      "d_state = 8\n",
      "d_conv = 4\n",
      "expand = 2\n",
      "n_facotrs = 4\n",
      "sess_dropout = 0.2\n",
      "item_dropout = 0.2\n",
      "temperature = 0.07\n",
      "Use_CustomDataset = True\n",
      "Use_CustomSampler = True\n",
      "single_spec = True\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "local_rank = 0\n",
      "device = cuda\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "22 Dec 19:13    INFO  foursquare_NYC\n",
      "The number of users: 1084\n",
      "Average actions of users: 208.99815327793166\n",
      "The number of items: 38334\n",
      "Average actions of items: 5.916897579338109\n",
      "The number of inters: 226345\n",
      "The sparsity of the dataset: 99.45529986290629%\n",
      "Remain Fields: ['user_id', 'venue_id', 'timestamp', 'venue_category_id', 'latitude', 'longitude', 'venue_id_list', 'timestamp_list', 'venue_seq_len']\n",
      "22 Dec 19:13    INFO  Mamba4POI(\n",
      "  (itemloc_embedding): Embedding(38334, 64)\n",
      "  (itemcat_embedding): Embedding(38334, 64)\n",
      "  (userloc_embedding): Embedding(1084, 64)\n",
      "  (usercat_embedding): Embedding(1084, 64)\n",
      "  (itembase_embedding): Embedding(38334, 64, padding_idx=0)\n",
      "  (userbase_embedding): Embedding(1084, 64, padding_idx=0)\n",
      "  (Norm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (LocMamba): ModuleList(\n",
      "    (0): MambaLayer(\n",
      "      (mamba): Mamba(\n",
      "        (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "        (act): SiLU()\n",
      "        (x_proj): Linear(in_features=128, out_features=20, bias=False)\n",
      "        (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (w_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (w_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (CatMamba): ModuleList(\n",
      "    (0): MambaLayer(\n",
      "      (mamba): Mamba(\n",
      "        (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "        (act): SiLU()\n",
      "        (x_proj): Linear(in_features=128, out_features=20, bias=False)\n",
      "        (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (w_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (w_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_fct): CrossEntropyLoss()\n",
      ")\n",
      "Trainable parameters: 5171456\n",
      "22 Dec 19:13    INFO  FLOPs: 4292736.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986cfe4e40c44ba18f49a1c06b79f41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112562822462577, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Modules.mamba4poi import Mamba4POI\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.quick_start import load_data_and_model\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "# 加载数据和模型配置\n",
    "    _,CORE, _, _, _, _ = load_data_and_model(\n",
    "    model_file='SavedData/Model/CORE-Dec-21-2024_20-59-51.pth'\n",
    ")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=Mamba4POI, config_file_list=['Configs/Mamba4POI.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model2 = Mamba4POI(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model2)\n",
    "    model2.itembase_embedding=CORE.item_embedding\n",
    "\n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model2, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model2)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import SRGNN\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=SRGNN, config_file_list=['Configs/SRGNN.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.model.sequential_recommender import SRGNN\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=SRGNN, config_file_list=['Configs/SRGNN.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = SRGNN(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert4rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import BERT4Rec\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=BERT4Rec, config_file_list=['Configs/BERT4Rec.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.model.sequential_recommender import BERT4Rec\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=BERT4Rec, config_file_list=['Configs/BERT4Rec.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = BERT4Rec(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SASRecF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trash!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import SASRecF\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=SASRecF, config_file_list=['Configs/SASRecF.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.model.sequential_recommender import SASRecF\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=SASRecF, config_file_list=['Configs/SASRecF.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = SASRecF(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEARec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import FEARec\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=FEARec, config_file_list=['Configs/FEARec.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.model.sequential_recommender import FEARec\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=FEARec, config_file_list=['Configs/FEARec.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = FEARec(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.general_recommender import DGCF\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=DGCF, config_file_list=['Configs/DGCF.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.model.general_recommender import DGCF\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=DGCF, config_file_list=['Configs/DGCF.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = DGCF(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGCF+Mamba4POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from Modules.dgcfMamba import Mamba4POI\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=Mamba4POI, config_file_list=['Configs/DGCFMamba.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.dgcfMamba import Mamba4POI\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.quick_start import load_data_and_model\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "# 加载数据和模型配置\n",
    "    _, DGCF, _, _, _, _ = load_data_and_model(\n",
    "    model_file='SavedData/Model/DGCF-Dec-20-2024_16-59-42.pth')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=Mamba4POI, config_file_list=['Configs/DGCFMamba.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model2 = Mamba4POI(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model2)\n",
    "    model2.itembase_embedding=DGCF.item_embedding\n",
    "    model2.userbase_embedding=DGCF.user_embedding\n",
    "    model2.factor=DGCF.n_factors\n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model2, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model2)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import SHAN\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=SHAN, config_file_list=['Configs/SHAN.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=SHAN, config_file_list=['Configs/SHAN.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = SHAN(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import GRU4Rec\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=GRU4Rec, config_file_list=['Configs/GRU4Rec.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=GRU4Rec, config_file_list=['Configs/GRU4Rec.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = GRU4Rec(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOSSIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import FOSSIL\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=FOSSIL, config_file_list=['Configs/FOSSIL.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=GRU4Rec, config_file_list=['Configs/FOSSIL.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = FOSSIL(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import HGN\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=FOSSIL, config_file_list=['Configs/HGN.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=GRU4Rec, config_file_list=['Configs/HGN.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = HGN(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import CORE\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=CORE, config_file_list=['Configs/CORE.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=CORE, config_file_list=['Configs/CORE.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = CORE(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COREMamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from Modules.coreMamba import Mamba4POI\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=Mamba4POI, config_file_list=['Configs/COREMamba.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.coreMamba import Mamba4POI\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.quick_start import load_data_and_model\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "# 加载数据和模型配置\n",
    "    _,CORE, _, _, _, _ = load_data_and_model(\n",
    "    model_file='SavedData/Model/CORE-Dec-21-2024_20-59-51.pth'\n",
    ")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=Mamba4POI, config_file_list=['Configs/COREMamba.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model2 = Mamba4POI(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model2)\n",
    "    model2.itembase_embedding=CORE.item_embedding\n",
    "\n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model2, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model2)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.coreMamba import Mamba4POI\n",
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.quick_start import load_data_and_model\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "# 加载数据和模型配置\n",
    "    _,CORE, _, _, _, _ = load_data_and_model(\n",
    "    model_file='SavedData/Model/CORE-Dec-21-2024_20-59-51.pth'\n",
    ")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=Mamba4POI, config_file_list=['Configs/COREMamba.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model2 = Mamba4POI(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model2)\n",
    "    model2.itembase_embedding=CORE.item_embedding\n",
    "\n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model2, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model2)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import SINE\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=SINE, config_file_list=['Configs/SINE.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=SINE, config_file_list=['Configs/SINE.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = SINE(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NextItNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import NextItNet\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=NextItNet, config_file_list=['Configs/NextItNet.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=NextItNet, config_file_list=['Configs/NextItNet.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = NextItNet(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from recbole.model.sequential_recommender import TransRec\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from Modules.myutils import * \n",
    "\n",
    "config = Config(model=TransRec, config_file_list=['Configs/TransRec.yaml'])\n",
    "dataset = create_dataset(config)\n",
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from recbole.trainer import Trainer\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    config = Config(model=TransRec, config_file_list=['Configs/TransRec.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = TransRec(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=False,\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
    "    trainer.wandblogger._wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_metrics_from_log(log_file):\n",
    "    # 初始化指标\n",
    "    epochs = []\n",
    "    train_losses = []\n",
    "    valid_scores = []\n",
    "    recall_1 = []\n",
    "    recall_5 = []\n",
    "    recall_10 = []\n",
    "    recall_20 = []\n",
    "    mrr_1 = []\n",
    "    mrr_5 = []\n",
    "    mrr_10 = []\n",
    "    mrr_20 = []\n",
    "    ndcg_1 = []\n",
    "    ndcg_5 = []\n",
    "    ndcg_10 = []\n",
    "    ndcg_20 = []\n",
    "    current_epoch = -1\n",
    "    with open(log_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            # 提取训练损失和验证得分\n",
    "            train_loss_match = re.search(r'train loss: ([\\d\\.]+)', line)\n",
    "            if train_loss_match:\n",
    "                train_losses.append(float(train_loss_match.group(1)))\n",
    "\n",
    "            valid_score_match = re.search(r'valid_score: ([\\d\\.]+)', line)\n",
    "            if valid_score_match:\n",
    "                valid_scores.append(float(valid_score_match.group(1)))\n",
    "\n",
    "            # 提取 recall、mrr、ndcg 等指标\n",
    "            result_match = re.search(r'recall@1 : ([\\d\\.]+).*?recall@5 : ([\\d\\.]+).*?recall@10 : ([\\d\\.]+).*?recall@20 : ([\\d\\.]+).*?mrr@1 : ([\\d\\.]+).*?mrr@5 : ([\\d\\.]+).*?mrr@10 : ([\\d\\.]+).*?mrr@20 : ([\\d\\.]+).*?ndcg@1 : ([\\d\\.]+).*?ndcg@5 : ([\\d\\.]+).*?ndcg@10 : ([\\d\\.]+).*?ndcg@20 : ([\\d\\.]+)', line)\n",
    "            if result_match:\n",
    "                recall_1.append(float(result_match.group(1)))\n",
    "                recall_5.append(float(result_match.group(2)))\n",
    "                recall_10.append(float(result_match.group(3)))\n",
    "                recall_20.append(float(result_match.group(4)))\n",
    "                mrr_1.append(float(result_match.group(5)))\n",
    "                mrr_5.append(float(result_match.group(6)))\n",
    "                mrr_10.append(float(result_match.group(7)))\n",
    "                mrr_20.append(float(result_match.group(8)))\n",
    "                ndcg_1.append(float(result_match.group(9)))\n",
    "                ndcg_5.append(float(result_match.group(10)))\n",
    "                ndcg_10.append(float(result_match.group(11)))\n",
    "                ndcg_20.append(float(result_match.group(12)))\n",
    "\n",
    "              # 提取 epoch 信息，避免重复记录\n",
    "            epoch_match = re.search(r'epoch (\\d+)', line)\n",
    "            if epoch_match:\n",
    "                epoch = int(epoch_match.group(1))\n",
    "                if epoch != current_epoch:  # 如果当前epoch和上一条记录的epoch不同，才记录\n",
    "                    epochs.append(epoch)\n",
    "                    current_epoch = epoch\n",
    "                \n",
    "    return {\n",
    "        'epochs': epochs,\n",
    "        'train_losses': train_losses,\n",
    "        'valid_scores': valid_scores,\n",
    "        'recall_1': recall_1,\n",
    "        'recall_5': recall_5,\n",
    "        'recall_10': recall_10,\n",
    "        'recall_20': recall_20,\n",
    "        'mrr_1': mrr_1,\n",
    "        'mrr_5': mrr_5,\n",
    "        'mrr_10': mrr_10,\n",
    "        'mrr_20': mrr_20,\n",
    "        'ndcg_1': ndcg_1,\n",
    "        'ndcg_5': ndcg_5,\n",
    "        'ndcg_10': ndcg_10,\n",
    "        'ndcg_20': ndcg_20,\n",
    "    }\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    # 绘制训练损失和验证得分\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # 训练损失\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(metrics['epochs'], metrics['train_losses'], label='Train Loss', marker='o', markersize=4)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Train Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 验证得分\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(metrics['epochs'], metrics['valid_scores'], label='Valid Score', marker='o', markersize=4)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Valid Score')\n",
    "    plt.title('Valid Score')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 绘制Recall@1, Recall@5, Recall@10, Recall@20\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(metrics['epochs'], metrics['recall_1'], label='Recall@1', marker='o', markersize=4)\n",
    "    plt.plot(metrics['epochs'], metrics['recall_5'], label='Recall@5', marker='o', markersize=4)\n",
    "    plt.plot(metrics['epochs'], metrics['recall_10'], label='Recall@10', marker='o', markersize=4)\n",
    "    plt.plot(metrics['epochs'], metrics['recall_20'], label='Recall@20', marker='o', markersize=4)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Recall@1, Recall@5, Recall@10, Recall@20')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 绘制MRR@1, MRR@5, MRR@10, MRR@20\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(metrics['epochs'], metrics['mrr_1'], label='MRR@1', marker='o', markersize=4)\n",
    "    plt.plot(metrics['epochs'], metrics['mrr_5'], label='MRR@5', marker='o', markersize=4)\n",
    "    plt.plot(metrics['epochs'], metrics['mrr_10'], label='MRR@10', marker='o', markersize=4)\n",
    "    plt.plot(metrics['epochs'], metrics['mrr_20'], label='MRR@20', marker='o', markersize=4)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MRR')\n",
    "    plt.title('MRR@1, MRR@5, MRR@10, MRR@20')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 显示图表\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "log_file_path = '/mnt/nvme0n1p2/Files/Code/Mamba/Mamba4POI/log/Mamba4POI/Valid/Location User Negsampler.log'  # 替换为你的日志文件路径\n",
    "metrics = extract_metrics_from_log(log_file_path)\n",
    "\n",
    "plot_metrics(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
